{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import datasets\n",
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq,\\\n",
    "    AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, HfArgumentParser\n",
    "# from helpers import prepare_dataset_code_summary, compute_rouge_and_bleu\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SummarizationPipeline, pipeline\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/Users/ryanchhong/.cache/huggingface/datasets/json/default-ead8d37a6184d20b/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
      "100%|██████████| 1/1 [00:00<00:00, 907.47it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset('json', data_files=\"./codesearchnet-corpus/python/final/jsonl/train/python_train_0.jsonl\")\n",
    "dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SummarizationPipeline\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class CustomSummaryGenerationPipeline(SummarizationPipeline):\n",
    "\n",
    "    def preprocess(self, inputs):\n",
    "        max_seq_length = self.tokenizer.model_max_length\n",
    "        \n",
    "        examples_with_prompts = list(map(lambda x: \"Summarize Python: \" + x, inputs))\n",
    "        # print(examples_with_prompts)\n",
    "        tokenized_examples = self.tokenizer(\n",
    "            examples_with_prompts,\n",
    "            truncation=True,\n",
    "            max_length=max_seq_length,\n",
    "            padding='max_length',\n",
    "            return_tensors = 'pt'\n",
    "        )\n",
    "        return tokenized_examples\n",
    "\n",
    "    def _forward(self, model_inputs):\n",
    "        outputs = self.model.generate(**model_inputs)\n",
    "        return {\"encoded_pred\": outputs}\n",
    "\n",
    "    def postprocess(self, model_outputs):\n",
    "        # print(model_outputs)\n",
    "        encoded_predictions = model_outputs[\"encoded_pred\"]\n",
    "        decoded_predictions = self.tokenizer.batch_decode(encoded_predictions, skip_special_tokens=True)\n",
    "        # decoded_predictions = [pred.strip() for pred in decoded_predictions]\n",
    "        return decoded_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pipeline = pipeline(\n",
    "    task = \"summarization\",\n",
    "    model = \"./single_line_summarization_model/\",\n",
    "    pipeline_class = CustomSummaryGenerationPipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 76959.71it/s]\n"
     ]
    }
   ],
   "source": [
    "store = []\n",
    "for out in tqdm(my_pipeline(KeyDataset(dataset, 'code')[:2])):\n",
    "    store.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This code snippet uses the urllib.parse.urlparse function to parse a URL into a dictionary of arguments, then uses the query string from that dictionary to parse into a dictionary of arguments, and finally converts the resulting dictionary into a dictionary of key-value pairs.\n",
      "This code parses a URL into a dictionary of arguments using the urllib.parse.urlparse library and then parses the URL query string into a dictionary. It also converts a dictionary into a dictionary of key-value pairs.\n"
     ]
    }
   ],
   "source": [
    "for line_summary in store:\n",
    "    line_summary = \"\\n\".join(line_summary)\n",
    "    gpt_response = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",\n",
    "        prompt=\"Summarize in a couple of sentences:\\n\\n\" + line_summary,\n",
    "        temperature=0.7,\n",
    "        max_tokens=64,\n",
    "        top_p=1.0,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0\n",
    "    )\n",
    "    print(gpt_response['choices'][0]['text'].strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
