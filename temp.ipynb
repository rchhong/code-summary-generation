{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import datasets\n",
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq,\\\n",
    "    AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, HfArgumentParser\n",
    "# from helpers import prepare_dataset_code_summary, compute_rouge_and_bleu\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SummarizationPipeline, pipeline\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /Users/ryanchhong/.cache/huggingface/datasets/json/default-3f5d1d0082396c1a/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 6026.30it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 452.90it/s]\n",
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /Users/ryanchhong/.cache/huggingface/datasets/json/default-3f5d1d0082396c1a/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 533.63it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset('json', data_files=\"./codesearchnet-corpus/python/final/jsonl/train/python_train_0.jsonl\")\n",
    "dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SummarizationPipeline\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class CustomSummaryGenerationPipeline(SummarizationPipeline):\n",
    "\n",
    "    def preprocess(self, inputs):\n",
    "        max_seq_length = self.tokenizer.model_max_length\n",
    "        \n",
    "        examples_with_prompts = list(map(lambda x: \"Summarize Python: \" + x, inputs))\n",
    "        # print(examples_with_prompts)\n",
    "        tokenized_examples = self.tokenizer(\n",
    "            examples_with_prompts,\n",
    "            truncation=True,\n",
    "            max_length=max_seq_length,\n",
    "            padding='max_length',\n",
    "            return_tensors = 'pt'\n",
    "        )\n",
    "        return tokenized_examples\n",
    "\n",
    "    def _forward(self, model_inputs):\n",
    "        outputs = self.model.generate(**model_inputs)\n",
    "        return {\"encoded_pred\": outputs}\n",
    "\n",
    "    def postprocess(self, model_outputs):\n",
    "        # print(model_outputs)\n",
    "        encoded_predictions = model_outputs[\"encoded_pred\"]\n",
    "        decoded_predictions = self.tokenizer.batch_decode(encoded_predictions, skip_special_tokens=True)\n",
    "        # decoded_predictions = [pred.strip() for pred in decoded_predictions]\n",
    "        return decoded_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pipeline = pipeline(\n",
    "    task = \"summarization\",\n",
    "    model = \"./single_line_summarization_model/\",\n",
    "    pipeline_class = CustomSummaryGenerationPipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 76959.71it/s]\n"
     ]
    }
   ],
   "source": [
    "store = []\n",
    "for out in tqdm(my_pipeline(KeyDataset(dataset, 'code')[:2])):\n",
    "    store.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [\n",
    "[\"get the metrics of a model in a web browser\", \"check if a sandbox is running\", \"convert a string `result` to a Python object\"],\n",
    "[\"create a dictionary `params_dict` with key `key` and timeoutSecs `10\", \"check if parameters in dictionary `params_dict` are updated with dictionary `kwargs`\", \"get json data from a key `key` in seconds\", \"send a json request to the API '3/Models.json' with timeout `timeout\", \"dump json object `result` to console\", \"check if a sandbox is running\", \"convert a string `result` to a Python object\"],\n",
    "[\"delete a model `key` from a database\", \"delete a model `key` from the server\", \"check if key 'f00b4r' exists in result\", \"delete a model `result` from a python python script\", \"convert a string `result` to a Python object\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input = [\"Summarize in a couple of sentences:\\n\\n\" + \"\\n\".join(line_summary) for line_summary in test]\n",
    "gpt_response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=input,\n",
    "    temperature=0.7,\n",
    "    max_tokens=64,\n",
    "    top_p=1.0,\n",
    "    frequency_penalty=0.0,\n",
    "    presence_penalty=0.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \"\\n\\nThis article describes how to use the Werkzeug library in Python to get the metrics of a model in a web browser, check if a sandbox is running, and convert a string to a Python object.\"\n",
      "    },\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \" `response`\\n\\nThis code creates a dictionary with a key and timeoutSecs, then updates the parameters with a kwargs dictionary. It then sends a json request to the API and dumps the response to console. It also checks if a sandbox is running, and converts the response string to a Python object.\"\n",
      "    },\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 2,\n",
      "      \"logprobs\": null,\n",
      "      \"text\": \"\\n\\nTo delete a model key from a database or server, you can use the delete() method. To check if a key exists in a result, use the in operator. To delete a model result from a Python script, use the del keyword. To convert a string to a Python object, use the ast.\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1681607282,\n",
      "  \"id\": \"cmpl-75lA2E7vPa5StynjjxMiprbUd9NSF\",\n",
      "  \"model\": \"text-davinci-003\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 172,\n",
      "    \"prompt_tokens\": 217,\n",
      "    \"total_tokens\": 389\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(gpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nth_fibonacci(n):\n",
    "    if(n < 0):\n",
    "        raise Exception('Invalid Input')\n",
    "    \n",
    "    if(n < 2):\n",
    "        return 1\n",
    "\n",
    "    curr = 1\n",
    "    prev = curr\n",
    "\n",
    "    for _ in range(n):\n",
    "        prev, curr= curr, curr + prev\n",
    "    \n",
    "    return curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nth_fibonacci(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
