{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.Completion.create(\n",
    "  model=\"text-davinci-003\",\n",
    "  prompt=\"Summarize:\\n\\n parse url `url` to a dictionary of arguments `url_data` using urllib.parse.urlparse to get dictionary of url arguments.  parse url query string `url_data.query` into dictionary `arg_dict` and store it in dictionary `args_list`.  convert a dictionary `arg_dict` into a dictionary of values for the key `arg` and value `value` from a dictionary to a dictionary'.\",\n",
    "  temperature=0.7,\n",
    "  max_tokens=64,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_code_summary(examples, tokenizer, max_seq_length=None):\n",
    "    max_seq_length = tokenizer.model_max_length if max_seq_length is None else max_seq_length\n",
    "\n",
    "    examples_with_prompts = list(map(lambda x: \"Summarize Python: \" + x, map(lambda x: x, examples['code'])))\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples_with_prompts,\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "        padding='max_length',\n",
    "        return_tensors = 'pt'\n",
    "    )\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KeyDataset(dataset, 'code')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_single_line_model = './single_line_summarization_model/'\n",
    "\n",
    "pipe = pipeline(\n",
    "    task = \"summarization\",\n",
    "    model = \"./single_line_summarization_model/\"\n",
    ")\n",
    "\n",
    "print(\"Preprocessing data... (this takes a little bit, should only happen once per dataset)\")\n",
    "dataset = datasets.load_dataset('json', data_files=\"./codesearchnet-corpus/python/final/jsonl/train/python_train_0.jsonl\")\n",
    "dataset = dataset['train']\n",
    "\n",
    "# print(dataset[0]['code'])\n",
    "\n",
    "for out in tqdm(pipe(KeyDataset(dataset, 'code'))):\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import datasets\n",
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq,\\\n",
    "    AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, HfArgumentParser\n",
    "# from helpers import prepare_dataset_code_summary, compute_rouge_and_bleu\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and tokenizer from the specified pretrained model/checkpoint\n",
    "pipe = pipeline(\n",
    "    task = \"summarization\",\n",
    "    model = \"./single_line_summarization_model/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = [\"def get_url_args(url):\\n    url_data = urllib.parse.urlparse(url)\\n\", \"    arg_dict = urllib.parse.parse_qs(url_data.query)\\n\", \"    return arg_dict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(dataset[0]['code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Pipeline\n",
    "\n",
    "\n",
    "class SummaryGenerationPipeline(Pipeline):\n",
    "    def _sanitize_parameters(self, **kwargs):\n",
    "        preprocess_kwargs = {}\n",
    "        if \"maybe_arg\" in kwargs:\n",
    "            preprocess_kwargs[\"maybe_arg\"] = kwargs[\"maybe_arg\"]\n",
    "        return preprocess_kwargs, {}, {}\n",
    "\n",
    "    def preprocess(self, inputs, maybe_arg=2):\n",
    "        model_input = Tensor(inputs[\"input_ids\"])\n",
    "        return {\"model_input\": model_input}\n",
    "\n",
    "    def _forward(self, model_inputs):\n",
    "        # model_inputs == {\"model_input\": model_input}\n",
    "        outputs = self.model(**model_inputs)\n",
    "        # Maybe {\"logits\": Tensor(...)}\n",
    "        return outputs\n",
    "\n",
    "    def postprocess(self, model_outputs):\n",
    "        best_class = model_outputs[\"logits\"].softmax(-1)\n",
    "        return best_class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
